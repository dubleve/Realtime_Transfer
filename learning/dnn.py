# -*- coding: utf-8 -*-
"""다층NN.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bIKOiP5kj8iGtgc9Accom_4PhZ4ehZpi
"""

#Import
import numpy as np
import pandas as pd
import tensorflow as tf

#Import datasets & set datasets

X_data = list()
Y_data = list()
def load(x, y, result):
  n_datasets = int(input('Please type the number of datasets : '))
  filename = input('Please type the name of files : ')
  path = './datasets/labeled_data'
  for i in range(2, n_datasets):
      file= pd.read_csv(path + filename + str(i) + '.csv')
      dataset = file.values.tolist()
      for row in dataset:
        rows =list()
        for column in row:
          rows.append(column.split())
        rowdata = [list(map(int,i)) for i in rows]
        x.append(rowdata[0])
        if(i>181):
          y.append([0, 1])
        else:
          y.append(result)

load(X_data, Y_data, [1, 0])
#load(X_data, Y_data, [0,1])

# Construct the model

global_step = tf.Variable(0, trainable=False, name='global_step')

X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

W1 = tf.Variable(tf.random_uniform([100, 50], -1., 1.))
W2 = tf.Variable(tf.random_uniform([50, 2], -1., 1.))
b1 = tf.Variable(tf.zeros([50]))
b2 = tf.Variable(tf.zeros([2]))


#W3 = tf.Variable(tf.random_uniform([25, 2], -1., 1.))
#b3 = tf.Variable(tf.zeros([25]))

L1 = tf.add(tf.matmul(X, W1), b1)
L1 = tf.nn.relu(L1)

#L2 = tf.add(tf.matmul(L1, W2), b2)
#L2 = tf.nn.relu(L2)

model = tf.add(tf.matmul(L1, W2), b2)

# Cross-Entropy
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))

# Training

optimizer = tf.train.AdamOptimizer(learning_rate=0.01)
train_op = optimizer.minimize(cost, global_step=global_step)

# Init the session of tf

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

saver = tf.train.Saver(tf.global_variables())

ckpt = tf.train.get_checkpoint_state('./model')
if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
  saver.restore(sess, ckpt.model_checkpoint_path)
else:
  sess.run(tf.global_variables_initializer())

for step in range(2):
  sess.run(train_op, feed_dict={X: X_data, Y: Y_data})
  
  print('Step: %d, ' % sess.run(global_step), 
        'Cost: %.3f' % sess.run(cost, feed_dict={X: X_data, Y: Y_data}))

saver.save(sess, './model/dnn.ckpt', global_step=global_step)

prediction = tf.argmax(model, axis=1)
target = tf.argmax(Y, axis=1)
print('예측값: ', sess.run(prediction, feed_dict={X: X_data}))
print('실제값: ', sess.run(target, feed_dict={Y: Y_data}))

# Print the Accuracy

is_correct = tf.equal(prediction, target)
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: X_data, Y: Y_data}))